version: '3'
services:

#---------------------------------------- Zookeeper ----------------------------------------------------

 # zookeeper:
 #   image: wurstmeister/zookeeper
 #   expose:
 #     - "2181"
 #   environment:
 #     ZOOKEEPER_CLIENT_PORT: 2181
 #   networks:
 #     - data_hub

#---------------------------------------- Redis --------------------------------------------------------

#  redis:
#    image: bitnami/redis:latest
#    ports:
#      - 6379:6379
#    networks:
#      - data_hub
      
#---------------------------------------- MQTT Broker and Bridge ---------------------------------------
      
 # emqx:
 #   image: emqx/emqx:4.2.7
 #   hostname: emqx
 #   restart: always
 #   ports:
 #     - "1883:1883"
 #     - "8084:8084"
 #     - "8883:8883"
 #     - "18083:18083"
 #   networks:
 #     - data_hub 

 # emqx2:   
 #   image: emqx/emqx:4.2.7
 #   restart: always
 #   environment:
 #     EMQX_NAME: "emqx"
 #     EMQX_HOST: "emqx2.iot-analytics.io"
 #     EMQX_CLUSTER__DISCOVERY: "static"
 #     EMQX_CLUSTER__STATIC__SEEDS: "emqx@emqx1.iot-analytics.io, emqx@emqx2.iot-analytics.io"
 #   networks:
 #     - data_hub
 
#  kafka-mqtt:
#    image: confluentinc/cp-kafka-mqtt:latest
#    restart: always
#    depends_on:
#      - kafka1  
#    environment:
#      KAFKA_MQTT_BOOTSTRAP_SERVERS:  PLAINTEXT://kafka1:19091
#      KAFKA_MQTT_LISTENERS: emqx:1883
#      KAFKA_MQTT_TOPIC_REGEX_LIST: mqtt:.*
#    networks:
#      - data_hub
      
      
#  mqtt2kafka:
#    image: marmaechler/mqtt2kafkabridge:latest
#    hostname: mqtt2kafka
#    depends_on:
#      - kafka1
#      - emqx
#    restart: always
#    environment:
#      KAFKA_BROKER_HOST: kafka1:19091
#      MQTT_BROKER_HOST: emqx:1883
#      CLIENT_ID: MQTT2Kafka
#      MQTT_TOPIC_FILTER: LENZDRGB611
#    volumes:
#      - ./volumes/mqtt2kafkabridge/logs:/opt/mqtt2kafkabridge/logs
#    networks:
#      - data_hub

#---------------------------------------- Kafka -----------------------------------------------------

#  kafka1:
#    image: wurstmeister/kafka
#    command: [start-kafka.sh]
#    restart: always
#    hostname: kafka1
#    expose:
#      - "9091"
#    environment:
#      KAFKA_CREATE_TOPICS: "LENZDRGB610:1:1" # topic:partition:replicas
#      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
#      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
#      KAFKA_LISTENERS: INSIDE://kafka1:19091,OUTSIDE://localhost:9091
#      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka1:19091,OUTSIDE://localhost:9091
#      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
#      KAFKA_BROKER_ID: 1
#      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
#    networks:
#      - data_hub
#    volumes:
#      - ./volumes/kafka1/data:/var/lib/kafka/data
#    depends_on:
#      - "zookeeper"
#    links:
#      - "zookeeper"##

#  kafka2:
#    image: wurstmeister/kafka
#    command: [start-kafka.sh]
#    restart: always
#    hostname: kafka2
#    expose:
#      - "9092"
#    environment:
#      KAFKA_CREATE_TOPICS: "LENZDRGB610:1:1" # topic:partition:replicas
#      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
#      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
#      KAFKA_LISTENERS: INSIDE://kafka2:19092,OUTSIDE://localhost:9092
#      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka2:19092,OUTSIDE://localhost:9092
#      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
#      KAFKA_BROKER_ID: 2
#      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
#    networks:
#      - data_hub
#    volumes:
#      - ./volumes/kafka2/data:/var/lib/kafka/data
#    depends_on:
#      - "zookeeper"
#    links:
#      - "zookeeper"#

#  kafka3:
#    image: wurstmeister/kafka
#    command: [start-kafka.sh]
#    restart: always
#    hostname: kafka3
#    expose:
#      - "9093"
#    environment:
#      KAFKA_CREATE_TOPICS: "LENZDRGB610:1:1" # topic:partition:replicas
#      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
#      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
#      KAFKA_LISTENERS: INSIDE://kafka3:19093,OUTSIDE://localhost:9093
#      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka3:19093,OUTSIDE://localhost:9093
#      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
#      KAFKA_BROKER_ID: 3
#      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
#    networks:
#      - data_hub
#    volumes:
#      - ./volumes/kafka3/data:/var/lib/kafka/data
#    depends_on:
#      - "zookeeper"
#    links:
#      - "zookeeper"
      
 # schema-registry:
 #   image: confluentinc/cp-schema-registry:5.4.0
 #   hostname: schema-registry
 #   container_name: schema-registry
 #   depends_on:
 #     - zookeeper
 #     - kafka1
 #   ports:
 #     - "8081:8081"
 #   environment:
 #     SCHEMA_REGISTRY_HOST_NAME: schema-registry
 #     SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'
 #     
 # kafka-connect:
 #   image: confluentinc/cp-kafka-connect:5.4.0
 #   hostname: kafka-connect
 #   container_name: kafka-connect
 #   depends_on:
 #     - broker
 #     - schema-registry
 #   ports:
 #     - "8083:8083"
 #   environment:
 #     CONNECT_BOOTSTRAP_SERVERS: "kafka1:19091"
 #     CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
 #     CONNECT_REST_PORT: 8083
 #     CONNECT_GROUP_ID: kafka-connect
 #     CONNECT_CONFIG_STORAGE_TOPIC: kafka-connect-configs
 #     CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
 #     CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
 #     CONNECT_OFFSET_STORAGE_TOPIC: kafka-connect-offsets
 #     CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
 #     CONNECT_STATUS_STORAGE_TOPIC: kafka-connect-status
 #     CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
 #     CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
 #     CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
 #     CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
 #     CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
 #     CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
 #     CONNECT_ZOOKEEPER_CONNECT: 'zookeeper:2181'
 #     CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
 #     CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR
 #   command:
 #       - /bin/bash
 #       - -c
 #       - /
 #         cd /usr/share/java/kafka-connect-mqtt/
 #         curl https://packages.confluent.io/maven/io/confluent/kafka-mqtt/6.1.0/kafka-mqtt-6.1.0.jar -o kafka-mqtt-6.1.0.jar
 #         /etc/confluent/docker/run 
 #         
 #         sleep infinity

 #   depends_on:
 #     - zookeeper
 #     - kafka1
 #     - emqx
 #   networks:
 #     - data_hub
 #     
 # kafka-connect-ui:
 #   image: landoop/kafka-connect-ui:latest
 #   hostname: kafka-connect-ui
 #   container_name: kafka-connect-ui
 #   ports:
 #     - "9001:8000"
 #   environment:
 #     CONNECT_URL: "kafka-connect:8083"
 #     PROXY: "true"
 #   depends_on:
 #     - kafka-connect
 #   restart: always
 #   networks:
 #     - data_hub
 #   
#  kafka-ui:
#    image: provectuslabs/kafka-ui
#    container_name: kafka-ui
#    ports:
#      - "9000:8080"
#    restart: always
#    environment:
#      KAFKA_CLUSTERS_0_NAME: local
#      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka1:19091
#      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
#    networks:
#      - data_hub
#    depends_on:
#      - kafka1
#      - kafka2
#      - kafka3
      
#---------------------------------------- ELK Stack ----------------------------------------------------

  elasticsearch:
    hostname: elasticsearch
    build:
      context: volumes/elasticsearch/
    volumes:
      - type: bind
        source: ./volumes/elasticsearch/config/elasticsearch.yml
        target: /usr/share/elasticsearch/config/elasticsearch.yml
        read_only: true
      - type: volume
        source: elasticsearch
        target: /usr/share/elasticsearch/data
    expose: 
      - "9200"
      - "9300"
    environment:
      ES_JAVA_OPTS: "-Xmx512m -Xms512m"
      ELASTIC_USERNAME: elastic
      ELASTIC_PASSWORD: changeme
      # Use single node discovery in order to disable production mode and avoid bootstrap checks.
      # see: https://www.elastic.co/guide/en/elasticsearch/reference/current/bootstrap-checks.html
      discovery.type: single-node
    networks:
      - batch_layer

#  logstash:
#    hostname: logstash
#    build:
#      context: volumes/logstash/
#    volumes:
#      - type: bind
#        source: ./volumes/logstash/config/logstash.yml
#        target: /usr/share/logstash/config/logstash.yml
#        read_only: true
#      - type: bind
#        source: ./volumes/logstash/pipeline
#        target: /usr/share/logstash/pipeline
#        read_only: true
#    expose:
#      - "5044"
#      - "5000/tcp"
#      - "5000/udp"
#      - "9600"
#    environment:
#      LS_JAVA_OPTS: "-Xmx512m -Xms512m"
#    networks:
#      - data_hub
#      - batch_layer
#    depends_on:
#      - elasticsearch
#      #- kafka1
#      #- kafka2
         #- kafka3

  kibana:
    hostname: kibana
    build:
      context: volumes/kibana/
    volumes:
      - type: bind
        source: ./volumes/kibana/config/kibana.yml
        target: /usr/share/kibana/config/kibana.yml
        read_only: true
      - type: bind
        source: ./volumes/kibana/config/wazuh.yml
        target: /usr/share/kibana/data/wazuh/config/wazuh.yml
    ports:
      - "5601:5601"
    networks:
      - batch_layer
    depends_on:
      - elasticsearch
      - wazuh
    

#---------------------------------------- Flink  -----------------------------------------------------------

#  pyflink:
#    hostname: pyflink
#    build:
#      context: ./volumes/flink/
#    volumes:
#      - ./volumes/flink/jobs:/opt/volumes/flink/jobs
#    networks:
#      - speed_layer
#    depends_on:
#      - jobmanager
#      - taskmanager
  
#  jobmanager:
#    image: pyflink/playgrounds:1.10.0
#    volumes:
#      - ./examples:/opt/examples
#    hostname: "jobmanager"
#    expose:
#      - "6123"
#    ports:
#      - "8333:8333"
#    command: jobmanager
#    environment:
#      - JOB_MANAGER_RPC_ADDRESS=jobmanager
#    networks:
#      - speed_layer
#      
#  taskmanager:
#    image: pyflink/playgrounds:1.10.0
#    volumes:
#      - ./examples:/opt/examples
#    expose:
#      - "6121"
#      - "6122"
#    depends_on:
#      - jobmanager
#    command: taskmanager
#    links:
#      - jobmanager:jobmanager
#    environment:
#      - JOB_MANAGER_RPC_ADDRESS=jobmanager
#    networks:
#      - speed_layer


#---------------------------------------- Wazuh ---------------------------------------------------------------
  wazuh:
    image: wazuh/wazuh-odfe:4.1.1
    hostname: wazuh
    restart: always
    ports:
      - "1514:1514"
      - "1515:1515"
      - "514:514/udp"
      - "55000:55000"
    environment:
      ELASTICSEARCH_URL: "http://elasticsearch:9200"
      ELASTIC_USERNAME: elastic
      ELASTIC_PASSWORD: changeme
      FILEBEAT_SSL_VERIFICATION_MODE: none  
      API_USERNAME: adminSSL                               # Wazuh API username
      API_PASSWORD: aDiQc&ixj9&    
    volumes:
      - ossec_api_configuration:/var/ossec/api/configuration
      - ossec_etc:/var/ossec/etc
      - ossec_logs:/var/ossec/logs
      - ossec_queue:/var/ossec/queue
      - ossec_var_multigroups:/var/ossec/var/multigroups
      - ossec_integrations:/var/ossec/integrations
      - ossec_active_response:/var/ossec/active-response/bin
      - ossec_agentless:/var/ossec/agentless
      - ossec_wodles:/var/ossec/wodles
      - filebeat_etc:/etc/filebeat
      - filebeat_var:/var/lib/filebeat
    depends_on:
      - elasticsearch
    networks:
      - batch_layer
#---------------------------------------- Volumes and Networks ------------------------------------------------
      
volumes:
  elasticsearch:
  ossec_api_configuration:
  ossec_etc:
  ossec_logs:
  ossec_queue:
  ossec_var_multigroups:
  ossec_integrations:
  ossec_active_response:
  ossec_agentless:
  ossec_wodles:
  filebeat_etc:
  filebeat_var:
networks:
  data_hub:
    driver: bridge
  batch_layer:
    driver: bridge
  speed_layer:
    driver: bridge

    
