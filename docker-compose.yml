version: '3'
services:

#---------------------------------------- Zookeeper ----------------------------------------------------

  zookeeper:
    image: wurstmeister/zookeeper
    container_name: zookeeper
    expose:
      - "2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    networks:
      - data_hub

#---------------------------------------- Redis --------------------------------------------------------

#  redis:
#    image: bitnami/redis:latest
#    ports:
#      - 6379:6379
#    networks:
#      - data_hub
      
#---------------------------------------- MQTT Broker and Bridge ---------------------------------------
      
  emqx:
    image: emqx/emqx:4.2.7
    hostname: emqx
    container_name: emqx
    restart: always
    ports:
      - "1883:1883"
      - "8084:8084"
      - "8883:8883"
      - "18083:18083"
    networks:
      - data_hub 

      
  mqtt2kafka:
    image: marmaechler/mqtt2kafkabridge:latest
    hostname: mqtt2kafka
    container_name: mqtt2kafka
    depends_on:
      - kafka1
      - emqx
    restart: always
    environment:
      KAFKA_BROKER_HOST: kafka1:19091
      MQTT_BROKER_HOST: emqx:1883
      CLIENT_ID: MQTT2Kafka
      MQTT_TOPIC_FILTER: LENZDRGB611
    volumes:
      - ./volumes/mqtt2kafkabridge/logs:/opt/mqtt2kafkabridge/logs
    networks:
      - data_hub

#---------------------------------------- Ditto (Digital Twin) -------------------------------------------

  ditto-mongodb:
    image: docker.io/mongo:4.2
    container_name: ditto_mongodb
    command: mongod --storageEngine wiredTiger --noscripting
    user: mongodb
    ports:
      - "27017:27017"
    networks:
      data_hub:
        aliases:
          - mongodb
      
  ditto-policies:
    image: docker.io/eclipse/ditto-policies:${DITTO_VERSION:-latest}
    mem_limit: 384m
    container_name: ditto_policies
    environment:
      - TZ=Europe/Berlin
      - INSTANCE_INDEX=1
      - BIND_HOSTNAME=0.0.0.0
      - OPENJ9_JAVA_OPTIONS=-XX:+ExitOnOutOfMemoryError -Xtune:virtualized -Xss512k -XX:MaxRAMPercentage=80 -Dakka.coordinated-shutdown.exit-jvm=on -Dakka.cluster.shutdown-after-unsuccessful-join-seed-nodes=120s
      - MONGO_DB_HOSTNAME=mongodb
    # Set additional configuration options here
    # -Dditto.policies...
    command: java -jar starter.jar
    networks:
      data_hub:
        aliases:
          - ditto-cluster
      
  ditto-things:
    image: docker.io/eclipse/ditto-things:${DITTO_VERSION:-latest}
    mem_limit: 384m
    container_name: ditto_things
    depends_on:
      - policies
    environment:
      - TZ=Europe/Berlin
      - INSTANCE_INDEX=1
      - BIND_HOSTNAME=0.0.0.0
      - OPENJ9_JAVA_OPTIONS=-XX:+ExitOnOutOfMemoryError -Xtune:virtualized -Xss512k -XX:MaxRAMPercentage=80 -Dakka.coordinated-shutdown.exit-jvm=on -Dakka.cluster.shutdown-after-unsuccessful-join-seed-nodes=120s
      - MONGO_DB_HOSTNAME=mongodb
    # Set additional configuration options here
    # -Dditto.things...
    command: java -jar starter.jar
    networks:
      data_hub:
        aliases:
          - ditto-cluster
      
  ditto-thingssearch:
    image: docker.io/eclipse/ditto-things-search:${DITTO_VERSION:-latest}
    mem_limit: 384m
    container_name: ditto_thingssearch
    depends_on:
      - ditto-policies
    environment:
      - TZ=Europe/Berlin
      - INSTANCE_INDEX=1
      - BIND_HOSTNAME=0.0.0.0
      - OPENJ9_JAVA_OPTIONS=-XX:+ExitOnOutOfMemoryError -Xtune:virtualized -Xss512k -XX:MaxRAMPercentage=80 -Dakka.coordinated-shutdown.exit-jvm=on -Dakka.cluster.shutdown-after-unsuccessful-join-seed-nodes=120s
      - MONGO_DB_HOSTNAME=mongodb
    # Set additional configuration options here
    # -Dditto.things-search...
    command: java -jar starter.jar
    networks:
      data_hub:
        aliases:
          - ditto-cluster
      
  ditto-concierge:
    image: docker.io/eclipse/ditto-concierge:${DITTO_VERSION:-latest}
    mem_limit: 384m
    container_name: ditto_concierge
    depends_on:
      - ditto-policies
    environment:
      - TZ=Europe/Berlin
      - INSTANCE_INDEX=1
      - BIND_HOSTNAME=0.0.0.0
      - OPENJ9_JAVA_OPTIONS=-XX:+ExitOnOutOfMemoryError -Xtune:virtualized -Xss512k -XX:MaxRAMPercentage=80 -Dakka.coordinated-shutdown.exit-jvm=on -Dakka.cluster.shutdown-after-unsuccessful-join-seed-nodes=120s
      - MONGO_DB_HOSTNAME=mongodb
    # Set additional configuration options here
    # -Dditto.concierge...
    command: java -jar starter.jar
    networks:
      data_hub:
        aliases:
          - ditto-cluster
      
  ditto-connectivity:
    image: docker.io/eclipse/ditto-connectivity:${DITTO_VERSION:-latest}
    mem_limit: 384m
    container_name: ditto_connectivity
    depends_on:
      - ditto-policies
      - ditto-concierge
    environment:
      - TZ=Europe/Berlin
      - INSTANCE_INDEX=1
      - BIND_HOSTNAME=0.0.0.0
      - OPENJ9_JAVA_OPTIONS=-XX:+ExitOnOutOfMemoryError -Xtune:virtualized -Xss512k -XX:MaxRAMPercentage=80 -Dakka.coordinated-shutdown.exit-jvm=on -Dakka.cluster.shutdown-after-unsuccessful-join-seed-nodes=120s
      - MONGO_DB_HOSTNAME=mongodb
    # Set additional configuration options here
    # -Dditto.connectivity...
    command: java -jar starter.jar
    networks:
      - data_hub
      
  ditto-gateway:
    image: docker.io/eclipse/ditto-gateway:${DITTO_VERSION:-latest}
    mem_limit: 384m
    container_name: ditto_gateway
    depends_on:
      - ditto-policies
      - ditto-concierge
    ports:
      - "8081:8080"
    environment:
      - TZ=Europe/Berlin
      - INSTANCE_INDEX=1
      - BIND_HOSTNAME=0.0.0.0
      - ENABLE_PRE_AUTHENTICATION=true
      - OPENJ9_JAVA_OPTIONS=-XX:+ExitOnOutOfMemoryError -Xtune:virtualized -Xss512k -XX:MaxRAMPercentage=80 -Dakka.coordinated-shutdown.exit-jvm=on -Dakka.cluster.shutdown-after-unsuccessful-join-seed-nodes=120s
      # You may use the environment for setting the devops password
      #- DEVOPS_PASSWORD=foobar
    # Set additional configuration options here
    # -Dditto.gateway...
    # Setting the devops password via java VM environment
    command: java -Dditto.gateway.authentication.devops.password=foobar -jar starter.jar
    networks:
      data_hub:
        aliases:
          - ditto-cluster
      
  ditto-swaggerui:
    image: docker.io/swaggerapi/swagger-ui:v3.38.0
    container_name: ditto_swaggerui
    volumes:
       - ./volumes/ditto/openapi:/usr/share/nginx/html/openapi:ro
       - ./volumes/ditto/images:/usr/share/nginx/html/images:ro
       - ./volumes/ditto/swagger3-index.html:/usr/share/nginx/html/index.html:ro
    command: nginx -g 'daemon off;'
    networks:
      - data_hub
      
  ditto-nginx:
    image: docker.io/nginx:1.19-alpine
    container_name: ditto_nginx
    volumes:
       - ./volumes/ditto/nginx.conf:/etc/nginx/nginx.conf:ro
       - ./volumes/ditto/nginx.htpasswd:/etc/nginx/nginx.htpasswd:ro
       - ./volumes/ditto/nginx-cors.conf:/etc/nginx/nginx-cors.conf:ro
       - ./volumes/ditto/index.html:/etc/nginx/html/index.html:ro
       - ./volumes/ditto/images:/etc/nginx/html/images:ro
    ports:
      - "${DITTO_EXTERNAL_PORT:-8080}:80"
    depends_on:
      - ditto-gateway
      - ditto-swaggerui
    networks:
      - data_hub
#---------------------------------------- Kafka -----------------------------------------------------

  kafka1:
    image: wurstmeister/kafka
    command: [start-kafka.sh]
    restart: always
    hostname: kafka1
    container_name: kafka1
    expose:
      - "9091"
    environment:
      KAFKA_CREATE_TOPICS: "LENZDRGB610:1:1" # topic:partition:replicas
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_LISTENERS: INSIDE://kafka1:19091,OUTSIDE://localhost:9091
      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka1:19091,OUTSIDE://localhost:9091
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_BROKER_ID: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      - data_hub
    volumes:
      - ./volumes/kafka1/data:/var/lib/kafka/data
    depends_on:
      - "zookeeper"
    links:
      - "zookeeper"

  kafka2:
    image: wurstmeister/kafka
    command: [start-kafka.sh]
    restart: always
    hostname: kafka2
    container_name: kafka2
    expose:
      - "9092"
    environment:
      KAFKA_CREATE_TOPICS: "LENZDRGB610:1:1" # topic:partition:replicas
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_LISTENERS: INSIDE://kafka2:19092,OUTSIDE://localhost:9092
      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka2:19092,OUTSIDE://localhost:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_BROKER_ID: 2
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      - data_hub
    volumes:
      - ./volumes/kafka2/data:/var/lib/kafka/data
    depends_on:
      - "zookeeper"
    links:
      - "zookeeper"

  kafka3:
    image: wurstmeister/kafka
    command: [start-kafka.sh]
    restart: always
    hostname: kafka3
    container_name: kafka3
    expose:
      - "9093"
    environment:
      KAFKA_CREATE_TOPICS: "LENZDRGB610:1:1" # topic:partition:replicas
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_LISTENERS: INSIDE://kafka3:19093,OUTSIDE://localhost:9093
      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka3:19093,OUTSIDE://localhost:9093
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_BROKER_ID: 3
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      - data_hub
    volumes:
      - ./volumes/kafka3/data:/var/lib/kafka/data
    depends_on:
      - "zookeeper"
    links:
      - "zookeeper"
     
 # schema-registry:
 #   image: confluentinc/cp-schema-registry:5.4.0
 #   hostname: schema-registry
 #   container_name: schema-registry
 #   depends_on:
 #     - zookeeper
 #     - kafka1
 #   ports:
 #     - "8081:8081"
 #   environment:
 #     SCHEMA_REGISTRY_HOST_NAME: schema-registry
 #     SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'
 #     
 # kafka-connect:
 #   image: confluentinc/cp-kafka-connect:5.4.0
 #   hostname: kafka-connect
 #   container_name: kafka-connect
 #   depends_on:
 #     - broker
 #     - schema-registry
 #   ports:
 #     - "8083:8083"
 #   environment:
 #     CONNECT_BOOTSTRAP_SERVERS: "kafka1:19091"
 #     CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
 #     CONNECT_REST_PORT: 8083
 #     CONNECT_GROUP_ID: kafka-connect
 #     CONNECT_CONFIG_STORAGE_TOPIC: kafka-connect-configs
 #     CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
 #     CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
 #     CONNECT_OFFSET_STORAGE_TOPIC: kafka-connect-offsets
 #     CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
 #     CONNECT_STATUS_STORAGE_TOPIC: kafka-connect-status
 #     CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
 #     CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
 #     CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
 #     CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
 #     CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
 #     CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
 #     CONNECT_ZOOKEEPER_CONNECT: 'zookeeper:2181'
 #     CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
 #     CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR
 #   command:
 #       - /bin/bash
 #       - -c
 #       - /
 #         cd /usr/share/java/kafka-connect-mqtt/
 #         curl https://packages.confluent.io/maven/io/confluent/kafka-mqtt/6.1.0/kafka-mqtt-6.1.0.jar -o kafka-mqtt-6.1.0.jar
 #         /etc/confluent/docker/run 
 #         
 #         sleep infinity

 #   depends_on:
 #     - zookeeper
 #     - kafka1
 #     - emqx
 #   networks:
 #     - data_hub
 #     
 # kafka-connect-ui:
 #   image: landoop/kafka-connect-ui:latest
 #   hostname: kafka-connect-ui
 #   container_name: kafka-connect-ui
 #   ports:
 #     - "9001:8000"
 #   environment:
 #     CONNECT_URL: "kafka-connect:8083"
 #     PROXY: "true"
 #   depends_on:
 #     - kafka-connect
 #   restart: always
 #   networks:
 #     - data_hub
 #   
 
  kafka-ui:
    image: provectuslabs/kafka-ui
    container_name: kafka-ui
    ports:
      - "9000:8080"
    restart: always
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka1:19091
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    networks:
      - data_hub
    depends_on:
      - kafka1
      - kafka2
      - kafka3
      
#---------------------------------------- ELK Stack ----------------------------------------------------

  elasticsearch:
    hostname: elasticsearch
    container_name: elasticsearch
    build:
      context: volumes/elasticsearch/
    volumes:
      - type: bind
        source: ./volumes/elasticsearch/config/elasticsearch.yml
        target: /usr/share/elasticsearch/config/elasticsearch.yml
        read_only: true
      - type: volume
        source: elasticsearch
        target: /usr/share/elasticsearch/data
    expose: 
      - "9200"
      - "9300"
    environment:
      ES_JAVA_OPTS: "-Xmx512m -Xms512m"
      ELASTIC_USERNAME: elastic
      ELASTIC_PASSWORD: changeme
      # Use single node discovery in order to disable production mode and avoid bootstrap checks.
      # see: https://www.elastic.co/guide/en/elasticsearch/reference/current/bootstrap-checks.html
      discovery.type: single-node
    networks:
      - batch_layer

  logstash:
    hostname: logstash
    container_name: logstash
    build:
      context: volumes/logstash/
    volumes:
      - type: bind
        source: ./volumes/logstash/config/logstash.yml
        target: /usr/share/logstash/config/logstash.yml
        read_only: true
      - type: bind
        source: ./volumes/logstash/pipeline
        target: /usr/share/logstash/pipeline
        read_only: true
    expose:
      - "5044"
      - "5000/tcp"
      - "5000/udp"
      - "9600"
    environment:
      LS_JAVA_OPTS: "-Xmx512m -Xms512m"
    networks:
      - data_hub
      - batch_layer
    depends_on:
      - elasticsearch
      - kafka1
      - kafka2
      - kafka3

  kibana:
    hostname: kibana
    container_name: kibana
    build:
      context: volumes/kibana/
    volumes:
      - type: bind
        source: ./volumes/kibana/config/kibana.yml
        target: /usr/share/kibana/config/kibana.yml
        read_only: true
      - type: bind
        source: ./volumes/kibana/config/wazuh.yml
        target: /usr/share/kibana/data/wazuh/config/wazuh.yml
    ports:
      - "5601:5601"
    networks:
      - batch_layer
    depends_on:
      - elasticsearch
      - wazuh
    

#---------------------------------------- Flink  -----------------------------------------------------------

#  pyflink:
#    hostname: pyflink
#    build:
#      context: ./volumes/flink/
#    volumes:
#      - ./volumes/flink/jobs:/opt/volumes/flink/jobs
#    networks:
#      - speed_layer
#    depends_on:
#      - jobmanager
#      - taskmanager
  
#  jobmanager:
#    image: pyflink/playgrounds:1.10.0
#    volumes:
#      - ./examples:/opt/examples
#    hostname: "jobmanager"
#    expose:
#      - "6123"
#    ports:
#      - "8333:8333"
#    command: jobmanager
#    environment:
#      - JOB_MANAGER_RPC_ADDRESS=jobmanager
#    networks:
#      - speed_layer
#      
#  taskmanager:
#    image: pyflink/playgrounds:1.10.0
#    volumes:
#      - ./examples:/opt/examples
#    expose:
#      - "6121"
#      - "6122"
#    depends_on:
#      - jobmanager
#    command: taskmanager
#    links:
#      - jobmanager:jobmanager
#    environment:
#      - JOB_MANAGER_RPC_ADDRESS=jobmanager
#    networks:
#      - speed_layer


#---------------------------------------- Wazuh ---------------------------------------------------------------
  wazuh:
    image: wazuh/wazuh-odfe:4.1.1
    hostname: wazuh
    container_name: wazuh
    restart: always
    ports:
      - "1514:1514"
      - "1515:1515"
      - "514:514/udp"
      - "55000:55000"
    environment:
      ELASTICSEARCH_URL: "http://elasticsearch:9200"
      ELASTIC_USERNAME: elastic
      ELASTIC_PASSWORD: changeme
      FILEBEAT_SSL_VERIFICATION_MODE: none  
      API_USERNAME: "adlheid"                               # Wazuh API username
      API_PASSWORD: "IacXSo?aDiQcixj9"  
    volumes:
      - ossec_api_configuration:/var/ossec/api/configuration
      - ossec_etc:/var/ossec/etc
      - ossec_logs:/var/ossec/logs
      - ossec_queue:/var/ossec/queue
      - ossec_var_multigroups:/var/ossec/var/multigroups
      - ossec_integrations:/var/ossec/integrations
      - ossec_active_response:/var/ossec/active-response/bin
      - ossec_agentless:/var/ossec/agentless
      - ossec_wodles:/var/ossec/wodles
      - filebeat_etc:/etc/filebeat
      - filebeat_var:/var/lib/filebeat
    depends_on:
      - elasticsearch
    networks:
      - batch_layer
#---------------------------------------- Volumes and Networks ------------------------------------------------
      
volumes:
  elasticsearch:
  ossec_api_configuration:
  ossec_etc:
  ossec_logs:
  ossec_queue:
  ossec_var_multigroups:
  ossec_integrations:
  ossec_active_response:
  ossec_agentless:
  ossec_wodles:
  filebeat_etc:
  filebeat_var:
networks:
  data_hub:
    driver: bridge
  batch_layer:
    driver: bridge
  speed_layer:
    driver: bridge

    
